{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# This function returns a dataframe with the packets times, sizes, and directions for a single row of data.\n",
    "# This will be used within our other functions to help create the features.\n",
    "def three_cols(row):\n",
    "    time = list(map(int, row['packet_times'].split(';')[0:-1]))\n",
    "    size = list(map(int, row['packet_sizes'].split(';')[0:-1]))\n",
    "    dirs = list(map(int, row['packet_dirs'].split(';')[0:-1]))\n",
    "    dict1 = {'packet_time': time, 'packet_size': size, 'packet_dir': dirs}\n",
    "    return pd.DataFrame(dict1)\n",
    "\n",
    "# This function takes all the counts of the 0-300bytes for the 1->2 Direction and all the counts\n",
    "# of the 1200-1500bytes for the 2->1 Direction and creates sum values for the two features per dataset.\n",
    "# uses the three_cols function as a helper function\n",
    "def big_byte_count_feature(dataset):        \n",
    "    packet_size_count1 = []\n",
    "    packet_size_count2 = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        row = three_cols(dataset.iloc[i])\n",
    "        ones = row.loc[row['packet_dir'] == 1]['packet_size']\n",
    "        twos = row.loc[row['packet_dir'] == 2]['packet_size']\n",
    "        one_count=0\n",
    "        two_count=0\n",
    "        for packet in ones:\n",
    "            if (int(packet) >= 0) and (int(packet) <= 300):\n",
    "                one_count += 1\n",
    "        for packet in twos:\n",
    "            if (int(packet) >= 1200) and (int(packet) <= 1500):\n",
    "                two_count += 1\n",
    "        packet_size_count1.append(one_count)\n",
    "        packet_size_count2.append(two_count)\n",
    "    return [sum(packet_size_count1), sum(packet_size_count2)]\n",
    "  \n",
    "  \n",
    "  # input: filepaths\n",
    "# output: 4 lists -> associated file names, labels, feature1, feature2\n",
    "# uses the big_byte_count_feature as a helper function\n",
    "def features_labels(filepath):\n",
    "    Dir1_ByteCount_0to300_feature = []\n",
    "    Dir2_ByteCount_1200to1500_feature = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    files = os.listdir(filepath)\n",
    "    for file in files:\n",
    "        if ('novpn' in file) or (file[:2] == '._'):\n",
    "            continue\n",
    "        if 'novideo' in file:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "        file_names.append(file)\n",
    "        df = pd.read_csv(filepath + '/' + file)\n",
    "        sum_values = big_byte_count_feature(df)\n",
    "        Dir1_ByteCount_0to300_feature.append(sum_values[0])\n",
    "        Dir2_ByteCount_1200to1500_feature.append(sum_values[1])\n",
    "    feature_df = pd.DataFrame(data={'Dir1_ByteCount_0to300_feature': Dir1_ByteCount_0to300_feature,\n",
    "                                    'Dir2_ByteCount_1200to1500_feature': Dir2_ByteCount_1200to1500_feature})\n",
    "    return file_names, labels, feature_df \n",
    "\n",
    "# accesses the data file found within the data folder and creates the features and label for it\n",
    "# uses the big_byte_count_feature as a helper function\n",
    "def input_feature_label(filepath):\n",
    "    Dir1_ByteCount_0to300_feature = []\n",
    "    Dir2_ByteCount_1200to1500_feature = []\n",
    "    labels = []\n",
    "    file_names = []\n",
    "    files = os.listdir(filepath)\n",
    "    for file in files:\n",
    "        if ('novpn' in file) or (file[:2] == '._'):\n",
    "            return \"File Invalid. Must be vpn data, nor can it be empty.\"\n",
    "        if 'novideo' in file:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "        file_names.append(file)\n",
    "        df = pd.read_csv(filepath + file)\n",
    "        sum_values = big_byte_count_feature(df)\n",
    "        Dir1_ByteCount_0to300_feature.append(sum_values[0])\n",
    "        Dir2_ByteCount_1200to1500_feature.append(sum_values[1])\n",
    "    feature_df = pd.DataFrame(data={'Dir1_ByteCount_0to300_feature': Dir1_ByteCount_0to300_feature,\n",
    "                                    'Dir2_ByteCount_1200to1500_feature': Dir2_ByteCount_1200to1500_feature})\n",
    "    return file_names, labels, feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "#Trains, tests, and splits the data up so that RandomForestClassifier can be used \n",
    "#to train on the data and then determine how accurate the model is\n",
    "def ml_model_analysis(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    model = model.fit(X_tr[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']],y_tr)\n",
    "    prediction_test = model.predict(X_ts[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    prediction_train = model.predict(X_tr[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    print ((\"Base test accuracy\", metrics.accuracy_score(y_ts, prediction_test)), \n",
    "            (\"Base Train Accuracy\", metrics.accuracy_score(y_tr, prediction_train)))\n",
    "    return prediction_test, y_ts\n",
    "\n",
    "  \n",
    "#Trains the model on all the data found within the GoodData on dsmlp, and then predicts \n",
    "#whether streaming or not for the input data chunk entered\n",
    "def ml_model_train(X, y, input_X, input_y, filename):\n",
    "    model = RandomForestClassifier()\n",
    "    model = model.fit(X[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']],y)\n",
    "    # save the model to temp/model folder\n",
    "    joblib.dump(model, filename)\n",
    "    return\n",
    "\n",
    "def classifer(input_model, filename):\n",
    "    loaded_model = joblib.load(filename)\n",
    "    prediction = loaded_model.predict(input_X[['Dir1_ByteCount_0to300_feature','Dir2_ByteCount_1200to1500_feature']])\n",
    "    for i in range(0, len(prediction)):\n",
    "        if bool(prediction[i]) == bool(input_y[i]):\n",
    "            val = \"Yes\"\n",
    "        else:\n",
    "            val = \"No\"\n",
    "        print(\"is_streaming? Prediction Value: \" + str(bool(prediction[i])), \"is_streaming? True Value: \" + str(bool(input_y[i])), \"classified correctly? : \" + val)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
